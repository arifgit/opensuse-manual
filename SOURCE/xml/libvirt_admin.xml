<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd">
<!--
*********************************
Please see LICENSE.txt for this document's license.
*********************************
-->
<chapter xml:base="libvirt_admin.xml" id="cha.libvirt.admin">
 <title>Administrating VM Guests</title>
 <para/>
 <sect1 id="sec.libvirt.admin.migrate">
  <title>Migrating VM Guests</title>

  <para>
   One of the major advantages of virtualization is the fact that VM Guests
   are portable. When a VM Host Server needs to go down for maintenance, or when
   the host gets overloaded, the guests can easily be moved to another
   VM Host Server. KVM and Xen even support <quote>live</quote> migrations
   during which the VM Guest is constantly available.
  </para>

  <para>
   In order to successfully migrate a VM Guest to another VM Host Server, the
   following requirements need to be met:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Host and target must have same processor manufacturer (Intel or AMD).
    </para>
   </listitem>

   <listitem os="osuse">
    <para>
     VM Guests running a 64-bit operating system can only be migrated to a
     64-bit host (whereas 32-bit VM Guests can be moved to a 64 or 32-bit
     host).
    </para>
   </listitem>
   <listitem>
    <para>
     Storage devices must be accessible from both machines (for example, via
     NFS or iSCSI) and must be configured as a storage pool on both machines
     (see <xref linkend="cha.libvirt.storage"/> for more information). This
     is also true for CD-ROM or floppy images that are connected during the
     move (however, you may disconnect them prior to the move as described
     in <xref linkend="sec.libvirt.config.cdrom.media_change"/>).
    </para>
   </listitem>
   <listitem>
    <para>
     <systemitem class="daemon">libvirtd</systemitem> needs to run on both VM Host Servers and you must be able to open
     a remote <systemitem class="library">libvirt</systemitem> connection between the target and the source host
     (or vice versa). Refer to
     <xref linkend="sec.libvirt.connect.remote"/> for details.
    </para>
   </listitem>
   <listitem>
    <para>
     If a firewall is running on the target host ports need to be opened to
     allow the migration. If you do not specify a port during the migration
     process, <systemitem class="library">libvirt</systemitem> chooses one from the range 49152:49215. Make sure
     that either this range (recommended) or a dedicated port of your choice
     is opened in the firewall on the <emphasis>target host</emphasis>.
    </para>
   </listitem>
   <listitem>
    <para>
     Host and target machine should be in the same subnet on the network,
     otherwise networking will not work after the migration.
    </para>
   </listitem>
   <listitem>
    <para>
     No running or paused VM Guest with the same name must exist on the
     target host. If a shut down machine with the same name exists, its
     configuration will be overwritten.
    </para>
   </listitem>
  </itemizedlist>

  <sect2 id="sec.libvirt.admin.migrate.virtmanager">
   <title>Migrating with <command>virt-manager</command></title>
   <para>
    When using the Virtual Machine Manager to migrate VM Guests, it does not matter on which
    machine it is started. You can start Virtual Machine Manager on the source or the target
    host or even on a third host. In the latter case you need to be able to
    open remote connections to both the target and the source host.
   </para>
   <procedure>
    <step performance="required">
     <para>
      Start Virtual Machine Manager and establish a connection to the target or the source
      host. If the Virtual Machine Manager was started neither on the target nor the source
      host, connections to both hosts need to be opened.
     </para>
    </step>
    <step performance="required">
     <para>
      Right-click on the VM Guest that is to be migrated and choose
      <guimenu>Migrate</guimenu>. Make sure the guest is running or paused -
      it is not possible to migrate guests that are shut off.
     </para>
    </step>
    <step performance="required">
     <para>
      Choose a <guimenu>New Host</guimenu> for the VM Guest. If the desired
      target host does not show up, make sure a connection to this host has
      been established.
     </para>
     <para>
      By default, a <quote>live</quote> migration is performed. If you
      prefer an <quote>offline</quote> migration where the VM Guest is
      paused during the migration, tick <guimenu>Migrate offline</guimenu>.
     </para>
    </step>
    <step performance="required">
     <para>
      Click <guimenu>Migrate</guimenu> to start a migration with the default
      port and bandwidth.
     </para>
     <para>
      In order to change these defaults, make the advanced options available
      by clicking the triangle at <guimenu>Advanced Options</guimenu>. Here
      you can enter the target host's <guimenu>Address</guimenu> (IP address
      or hostname), a port and the bandwidth in megabit per second (Mbps).
      If you specify a <guimenu>Port</guimenu>, you must also specify an
      <guimenu>Address</guimenu>; the <guimenu>Bandwidth</guimenu> is
      optional.
     </para>
    </step>
    <step performance="required">
     <para>
      Once the migration is complete, the <guimenu>Migrate</guimenu> window
      closes and the VM Guest is now listed on the new host in the Virtual Machine Manager
      Window. The original VM Guest will still be available on the target
      host (in state shut off).
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 id="sec.libvirt.admin.migrate.virsh">
   <title>Migrating with <command>virsh</command></title>
   <para>
    To migrate a VM Guest with <command>virsh
    <option>migrate</option></command>, you need to have direct or remote
    shell access to the VM Host Server, because the command needs to be run on the
    host. Basically the migration command looks like this
   </para>
   <screen>virsh migrate [OPTIONS] <replaceable>VM_ID_or_NAME</replaceable><replaceable>CONNECTION URI</replaceable> [--migrateuri tcp://<replaceable>REMOTE_HOST:PORT</replaceable>]</screen>
   <para>
    The most important options are listed below. See <command>virsh help
    migrate</command> for a full list.
   </para>
   <variablelist>
    <varlistentry>
     <term><option>--live</option>
     </term>
     <listitem>
      <para>
       Does a live migration. If not specified, an offline migration where
       the VM Guest is paused during the migration, will be performed.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>--suspend</option>
     </term>
     <listitem>
      <para>
       Does an offline migration and does not restart the VM Guest on the
       target host.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>--persistent</option>
     </term>
     <listitem>
      <para>
       By default a migrated VM Guest will be migrated transient, so its
       configuration is automatically deleted on the target host if it is
       shut down. Use this switch to make the migration persistent.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>--undefinesource</option>
     </term>
     <listitem>
      <para>
       When specified, the VM Guest definition on the source host will be
       deleted after a successful migration (however, virtual disks attached
       to this guest will <emphasis>not</emphasis> be deleted).
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    The following examples use mercury.example.com as the source system and jupiter.example.com
    as the target system, the VM Guest's name is
    <literal>opensuse11</literal> with Id <literal>37</literal>.
   </para>
   <variablelist>
    <varlistentry>
     <term>Offline migration with default parameters</term>
     <listitem>
      <screen>virsh migrate 37 qemu+ssh://root@jupiter.example.com/system</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Transient live migration with default parameters</term>
     <listitem>
      <screen>virsh migrate --live opensuse11 qemu+ssh://root@jupiter.example.com/system</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Persistent live migration; delete VM definition on source</term>
     <listitem>
      <screen>virsh migrate --live --persistent --undefinesource 37 \
qemu+tls://root@jupiter.example.com/system</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Offline migration using port 49152</term>
     <listitem>
      <screen>virsh migrate opensuse11 qemu+ssh://root@jupiter.example.com/system \
--migrateuri tcp://@jupiter.example.com:49152</screen>
     </listitem>
    </varlistentry>
   </variablelist>
   <note>
    <title>Transient vs. Persistant Migrations</title>
    <para>
     By default <command>virsh migrate</command> creates a temporary
     (transient) copy of the VM Guest on the target host. A shut down
     version of the original guest description remains on the source host. A
     transient copy will be deleted from the server once it is shut down.
    </para>
    <para>
     In order to create a permanent copy of a guest on the target host, use
     the switch <option>--persistent</option>. A shut down version of the
     original guest description remains on the source host, too. Use the
     option <option>--undefinesource</option> together with
     <option>--persistent</option> for a <quote>real</quote> move where a
     permanet copy is created on the target host and the version on the
     source host is deleted.
    </para>
    <para>
     It is not recommended to use <option>--undefinesource</option> without
     the <option>--persistent</option> option, since this will result in the
     loss of both VM Guest definitions when the guest is shut down on the
     target host.
    </para>
   </note>
  </sect2>
 </sect1>
 <sect1 id="cha.libvirt.admin.monitor">
  <title>Monitoring</title>

  <para/>

  <sect2 id="cha.libvirt.admin.monitor.virt-manager">
   <title>Monitoring with Virtual Machine Manager</title>
   <para>
    After starting Virtual Machine Manager and connecting to the VM Host Server, a CPU usage graph
    of all the running guests is displayed.
   </para>
   <para>
    It is also possible to get information about disk and network usage with
    this tool, however, you must first activate this in the
    <guimenu>Preferences</guimenu>:
   </para>
   <procedure>
    <step performance="required">
     <para>
      Run <command>virt-manager</command>.
     </para>
    </step>
    <step performance="required">
     <para>
      Select <menuchoice><guimenu>Edit</guimenu>
      <guimenu>Preferences</guimenu></menuchoice>.
     </para>
    </step>
    <step performance="required">
     <para>
      Change the tab from <guimenu>General</guimenu> to
      <guimenu>Stats</guimenu>.
     </para>
    </step>
    <step performance="required">
     <para>
      Activate the check boxes for <guimenu>Disk I/O</guimenu> and
      <guimenu>Network I/O</guimenu>.
     </para>
    </step>
    <step performance="required">
     <para>
      If desired, also change the update interval or the number of samples
      that are kept in the history.
     </para>
    </step>
    <step performance="required">
     <para>
      Close the <guimenu>Preferences</guimenu> dialog.
     </para>
    </step>
    <step performance="required">
     <para>
      Activate the graphs that should be displayed under <menuchoice>
      <guimenu>View</guimenu> <guimenu>Graph</guimenu> </menuchoice>.
     </para>
    </step>
   </procedure>
   <para>
    Afterwards, the disk and network statistics are also displayed in the
    main window of the Virtual Machine Manager.
   </para>
   <para>
    More precise data is available from the VNC window. Open a VNC window as
    described in <xref linkend="sec.libvirt.managing.vnc"/>. Choose
    <guimenu>Details</guimenu> from the toolbar or the
    <guimenu>View</guimenu> menu. The statistics are displayed from the
    <guimenu>Performance</guimenu> entry of the left-hand tree menu.
   </para>
  </sect2>

  <sect2>
   <title>Monitoring with <command>kvm_stat</command></title>
   <para>
    <command>kvm_stat</command> can be used to trace KVM performance
    events. It monitors <filename>/sys/kernel/debug/kvm</filename>, so it
    needs the debugfs to be mounted. On <phrase os="osuse">openSUSE</phrase> it should be mounted
    by default. In case it is not mounted, use the following command:
   </para>
   <screen>mount -t debugfs none /sys/kernel/debug</screen>
   <para>
    <command>kvm_stat</command> can be used in three different modes:
   </para>
   <screen>kvm_stat                    # update in 1 second intervals
kvm_stat -1                 # 1 second snapshot
kvm_stat -l &gt; kvmstats.log  # update in 1 second intervals in log format
                            # can be imported to a spreadsheet</screen>
   <example>
    <title>Typical Output of <command>kvm_stat</command></title>
    <screen>kvm statistics

 efer_reload                  0       0
 exits                 11378946  218130
 fpu_reload               62144     152
 halt_exits              414866     100
 halt_wakeup             260358      50
 host_state_reload       539650     249
 hypercalls                   0       0
 insn_emulation         6227331  173067
 insn_emulation_fail          0       0
 invlpg                  227281      47
 io_exits                113148      18
 irq_exits               168474     127
 irq_injections          482804     123
 irq_window               51270      18
 largepages                   0       0
 mmio_exits                6925       0
 mmu_cache_miss           71820      19
 mmu_flooded              35420       9
 mmu_pde_zapped           64763      20
 mmu_pte_updated              0       0
 mmu_pte_write           213782      29
 mmu_recycled                 0       0
 mmu_shadow_zapped       128690      17
 mmu_unsync                  46      -1
 nmi_injections               0       0
 nmi_window                   0       0
 pf_fixed               1553821     857
 pf_guest               1018832     562
 remote_tlb_flush        174007      37
 request_irq                  0       0
 signal_exits                 0       0
 tlb_flush               394182     148</screen>
   </example>
   <para>
    See
    <ulink url="http://clalance.blogspot.com/2009/01/kvm-performance-tools.html"/>
    for further information on how to interpret these values.
   </para>
  </sect2>
 </sect1>

</chapter>
